{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23d6854a",
   "metadata": {},
   "source": [
    "### Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "399bbf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report, accuracy_score, matthews_corrcoef\n",
    "from transformers import (set_seed,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          GPT2Config,\n",
    "                          GPT2Tokenizer,\n",
    "                          AdamW, \n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          GPT2ForSequenceClassification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f58139",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a48deb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (C:/Users/NIT/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19873d1ed517444da75b8d6bd98bd798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 8551\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1043\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1063\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('glue', 'cola')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7d394d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(123)\n",
    "epochs = 5\n",
    "\n",
    "batch_size = 32\n",
    "max_length = 120\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name_or_path = 'gpt2'\n",
    "\n",
    "labels_ids = {'neg': 0, 'pos': 1}\n",
    "\n",
    "n_labels = len(labels_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d119f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gpt2ClassificationCollator(object):\n",
    "    def __init__(self, use_tokenizer,  max_sequence_len=None):\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
    "\n",
    "    def __call__(self, sequences):\n",
    "        sentences = [sequence['sentence'] for sequence in sequences]\n",
    "        labels = [sequence['label'] for sequence in sequences]\n",
    "        inputs = self.use_tokenizer(text=sentences, return_tensors=\"pt\", padding=True, truncation=True,  max_length=self.max_sequence_len)\n",
    "        inputs.update({'labels':torch.tensor(labels)})\n",
    "        return inputs\n",
    "\n",
    "\n",
    "def train(dataloader, optimizer_, scheduler_, device_):\n",
    "    global model\n",
    "    predictions_labels = []\n",
    "    true_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "\n",
    "        batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss, logits = outputs[:2]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer_.step()\n",
    "        scheduler_.step()\n",
    "\n",
    "        predictions_labels.extend(torch.argmax(logits, dim = 1).tolist())\n",
    "\n",
    "    avg_epoch_loss = total_loss / len(dataloader)\n",
    "    return true_labels, predictions_labels, avg_epoch_loss\n",
    "\n",
    "\n",
    "def validation(dataloader, device_):\n",
    "    global model\n",
    "    predictions_labels = []\n",
    "    true_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "        batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**batch)\n",
    "\n",
    "            loss, logits = outputs[:2] \n",
    "            #logits = logits.detach().cpu().numpy()\n",
    "            total_loss += loss.item()\n",
    "            predict_content = torch.argmax(logits, dim = 1).tolist()\n",
    "            predictions_labels.extend(predict_content)\n",
    "\n",
    "    avg_epoch_loss = total_loss / len(dataloader)\n",
    "    return true_labels, predictions_labels, avg_epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d304437",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43cdc8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuraiton...\n",
      "Loading tokenizer...\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to `cuda`\n"
     ]
    }
   ],
   "source": [
    "# Get model configuration.\n",
    "print('Loading configuraiton...')\n",
    "model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path=model_name_or_path, num_labels=n_labels)\n",
    "\n",
    "# Get model's tokenizer.\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path=model_name_or_path)\n",
    "# default to left padding\n",
    "tokenizer.padding_side = \"right\"\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print('Loading model...')\n",
    "model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path, config=model_config)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.to(device)\n",
    "print('Model loaded to `%s`'%device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f2e153",
   "metadata": {},
   "source": [
    "### Creating train, validation and test dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "994c05a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `train_dataloader` with 268 batches!\n",
      "\n",
      "Created `val_dataloader` with 33 batches!\n",
      "Created `test_dataloader` with 34 batches!\n"
     ]
    }
   ],
   "source": [
    "# Create data collator to encode text and labels into numbers.\n",
    "gpt2_classificaiton_collator = Gpt2ClassificationCollator(use_tokenizer=tokenizer,  \n",
    "                                                          max_sequence_len=max_length)\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "train_dataloader = DataLoader(dataset['train'], batch_size=batch_size, shuffle=True, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `train_dataloader` with %d batches!'%len(train_dataloader))\n",
    "\n",
    "print()\n",
    "\n",
    "valid_dataloader = DataLoader(dataset['validation'], batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `val_dataloader` with %d batches!'%len(valid_dataloader))\n",
    "\n",
    "test_dataloader = DataLoader(dataset['test'], batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `test_dataloader` with %d batches!'%len(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b768c8",
   "metadata": {},
   "source": [
    "### Fine-tuning GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "631ce955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NIT\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a6f91674ef45a3a0d69abface065ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c2166a1f86434fa6d8475efa814cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d146c5e25bb24d239d8a08cffc339a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train_loss: 0.61005 - val_loss: 0.60114 - train_acc: 0.69407 - valid_acc: 0.68840\n",
      "\n",
      "\n",
      "Training on batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b93a5e11554264bd8455153307ae54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6fb36cd86a4bda9ede682badc0f857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train_loss: 0.54964 - val_loss: 0.52753 - train_acc: 0.72120 - valid_acc: 0.74017\n",
      "\n",
      "\n",
      "Training on batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219a388b4db9453ab083586f3e7510b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c1cd9fac084b6cbb4d5eecb57ede98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train_loss: 0.48446 - val_loss: 0.51829 - train_acc: 0.76821 - valid_acc: 0.76031\n",
      "\n",
      "\n",
      "Training on batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013aca8cab48426badcf1e7515c51c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011e32b7e7f1460eba0b47316d4afaee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train_loss: 0.42613 - val_loss: 0.53681 - train_acc: 0.80201 - valid_acc: 0.76318\n",
      "\n",
      "\n",
      "Training on batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651c592248e84dd8a25f20c688966b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76352f7979dd4edebf35b4ac9ac76204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train_loss: 0.39199 - val_loss: 0.55326 - train_acc: 0.82622 - valid_acc: 0.76222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "all_loss = {'train_loss':[], 'val_loss':[]}\n",
    "all_acc = {'train_acc':[], 'val_acc':[]}\n",
    "\n",
    "# Loop through each epoch.\n",
    "print('Epoch')\n",
    "best_valid_acc = 0\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print()\n",
    "    print('Training on batches...')\n",
    "    # Perform one full pass over the training set.\n",
    "    train_labels, train_predict, train_loss = train(train_dataloader, optimizer, scheduler, device)\n",
    "    train_acc = accuracy_score(train_labels, train_predict)\n",
    "\n",
    "    # Get prediction form model on validation data. \n",
    "    print('Validation on batches...')\n",
    "    valid_labels, valid_predict, val_loss = validation(valid_dataloader, device)\n",
    "    val_acc = accuracy_score(valid_labels, valid_predict)\n",
    "\n",
    "    # Print loss and accuracy values to see how training evolves.\n",
    "    print(\"  train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f\"%(train_loss, val_loss, train_acc, val_acc))\n",
    "    print()\n",
    "\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    all_loss['train_loss'].append(train_loss)\n",
    "    all_loss['val_loss'].append(val_loss)\n",
    "    all_acc['train_acc'].append(train_acc)\n",
    "    all_acc['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Check if the current model is the best so far\n",
    "    if val_acc > best_valid_acc:\n",
    "        best_valid_acc = val_acc\n",
    "        model.save_pretrained(\"gpt2-cola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a3718",
   "metadata": {},
   "outputs": [],
   "source": [
    "preTrainedModel = GPT2ForSequenceClassification.from_pretrained('gpt2')\n",
    "preTrainedModel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf40c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fineTunedModel = GPT2ForSequenceClassification.from_pretrained('gpt2-cola')\n",
    "fineTunedModel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937597ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44124eba",
   "metadata": {},
   "source": [
    "### Obtaining hidden states averaged over all tokens to get a single dimension of 768 for each example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66448513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHiddenStates(currModel, dataloader):\n",
    "    currModel.eval()\n",
    "    totalHiddenStates = [0]*13\n",
    "    flag = True\n",
    "    \n",
    "    # Evaluate data for one epoch\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        #token_type_ids = batch['token_type_ids'].to(device)\n",
    "        \n",
    "        with torch.no_grad():       \n",
    "            outputs = currModel(input_ids=input_ids, \n",
    "                                     attention_mask=attention_mask, \n",
    "                                     output_hidden_states=True, \n",
    "                                     return_dict=True)\n",
    "            hidden_states = outputs.hidden_states\n",
    "            if flag == True:\n",
    "                for i in range(len(hidden_states)):\n",
    "                    totalHiddenStates[i] = hidden_states[i].to(\"cpu\").mean(dim = 1)\n",
    "                flag = False\n",
    "            else:\n",
    "                for i in range(len(hidden_states)):\n",
    "                    temp = hidden_states[i].to(\"cpu\").mean(dim = 1)\n",
    "                    totalHiddenStates[i] = torch.cat((totalHiddenStates[i], temp), 0)   \n",
    "    return totalHiddenStates     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed558f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddenStates = getHiddenStates(model, test_dataloader)\n",
    "preTrainedHiddenStates = torch.stack(hiddenStates, dim = 0)\n",
    "torch.save(preTrainedHiddenStates, 'preTrainedHiddenStates_CoLA.pt')\n",
    "preTrainedHiddenStates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291373c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddenStates = getHiddenStates( fineTunedModel, test_dataloader)\n",
    "fineTunedHiddenStates = torch.stack(hiddenStates, dim = 0)\n",
    "torch.save(fineTunedHiddenStates, 'fineTunedHiddenStates_CoLA.pt')\n",
    "fineTunedHiddenStates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c353eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CKA import CKA, CudaCKA\n",
    "cuda_cka = CudaCKA(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b527650",
   "metadata": {},
   "outputs": [],
   "source": [
    "cka_matrix = torch.empty((13, 13))\n",
    "for i in range(13):\n",
    "    for j in range(13):\n",
    "        X = preTrainedHiddenStates[i].to(device)\n",
    "        Y = fineTunedHiddenStates[j].to(device)\n",
    "        cka_matrix[i][j] = cuda_cka.linear_CKA(X, Y)\n",
    "#torch.save(cka_matrix, 'ckaMatrix.pt')\n",
    "cka_matrix = cka_matrix.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbfffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(cka_matrix, cmap=\"inferno\", vmin=0.0, vmax=1.0)\n",
    "\n",
    "plt.xticks(np.arange(0,13), fontsize = 18)\n",
    "plt.yticks(np.arange(0,13), fontsize = 18)\n",
    "\n",
    "cka_matrix = np.round(cka_matrix, 2)\n",
    "for x in range(13):\n",
    "    for y in range(13):\n",
    "        plt.annotate(cka_matrix[x][y], xy=(y, x), horizontalalignment='center', verticalalignment='center', fontsize=13)\n",
    "\n",
    "plt.ylabel('Pre-trained GPT-2', fontsize = 22)\n",
    "plt.xlabel('Fine-tuned GPT-2', fontsize = 22)\n",
    "\n",
    "#fig.colorbar(im)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "plt.savefig('CoLA_preTrained_fineTuned.pdf',  bbox_inches='tight')\n",
    "plt.savefig('CoLA_preTrained_fineTuned.jpg',  bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff9db29",
   "metadata": {},
   "source": [
    "### Calculating layerwise STIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a8eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preTrainedHiddenStates = torch.load('./preTrainedHiddenStates_CoLA.pt').to(device)\n",
    "fineTunedHiddenStates  = torch.load('./fineTunedHiddenStates_CoLA.pt').to(device)\n",
    "print(preTrainedHiddenStates.shape,fineTunedHiddenStates.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e6b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinm1 = []\n",
    "distinm2 = []\n",
    "for i in range(13):\n",
    "    distinm1.append(torch.cdist(preTrainedHiddenStates[i], preTrainedHiddenStates[i]))\n",
    "    distinm2.append(torch.cdist(fineTunedHiddenStates[i], fineTunedHiddenStates[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7868cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(13):\n",
    "    for j in range(distinm1[0].shape[0]):\n",
    "        distinm1[i][j][j] = 100000.0\n",
    "        distinm2[i][j][j] = 100000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2728ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = preTrainedHiddenStates.shape[1]\n",
    "minDistIndForm1 = []\n",
    "for i in range(13):\n",
    "    temp = []\n",
    "    for j in range(num_rows):\n",
    "        temp.append(torch.argmin(distinm1[i][j]))\n",
    "    minDistIndForm1.append(temp)\n",
    "    \n",
    "minDistIndForm2 = []\n",
    "for i in range(13):\n",
    "    temp = []\n",
    "    for j in range(num_rows):\n",
    "        temp.append(torch.argmin(distinm2[i][j]))\n",
    "    minDistIndForm2.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f35ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CKA import CKA, CudaCKA\n",
    "cuda_cka = CudaCKA(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7d5234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def calculate(noOfSamples, noOfTimes):\n",
    "    STIRm2m1 = []\n",
    "    STIRm1m2 = []\n",
    "    CKA = []\n",
    "    for i in range(13):\n",
    "        stirm2m1 = 0\n",
    "        stirm1m2 = 0\n",
    "        cka = 0\n",
    "        for j in range(noOfTimes):\n",
    "            num_list = random.sample(range(0, num_rows), noOfSamples)\n",
    "            X = []\n",
    "            X_ = []\n",
    "            Y = []\n",
    "            Y_ = []\n",
    "            for k in range(len(num_list)):\n",
    "                X.append(fineTunedHiddenStates[i][num_list[k]])\n",
    "                X_.append(fineTunedHiddenStates[i][minDistIndForm1[i][num_list[k]]])\n",
    "                Y.append(preTrainedHiddenStates[i][num_list[k]])\n",
    "                Y_.append(preTrainedHiddenStates[i][minDistIndForm2[i][num_list[k]]])\n",
    "                \n",
    "            X = torch.stack(X, dim = 0)\n",
    "            X_ = torch.stack(X_, dim = 0)\n",
    "            Y = torch.stack(Y, dim = 0)\n",
    "            Y_ = torch.stack(Y_, dim = 0)\n",
    "            \n",
    "            stirm2m1 = stirm2m1 + cuda_cka.linear_CKA(X, X_)\n",
    "            stirm1m2 = stirm1m2 + cuda_cka.linear_CKA(Y, Y_)\n",
    "            cka = cka + cuda_cka.linear_CKA(Y, X)\n",
    "        \n",
    "        STIRm2m1.append(stirm2m1/noOfTimes)\n",
    "        STIRm1m2.append(stirm1m2/noOfTimes)\n",
    "        CKA.append(cka/noOfTimes)\n",
    "    return STIRm2m1, STIRm1m2, CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260681cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "STIRm2m1, STIRm1m2, CKA = calculate(num_rows//2, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98633f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "STIRm2m1 = torch.stack(STIRm2m1, dim = 0)\n",
    "STIRm1m2 = torch.stack(STIRm1m2,dim = 0)\n",
    "CKA = torch.stack(CKA, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7924cdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "STIRm2m1 = STIRm2m1.to(\"cpu\")\n",
    "STIRm2m1 = STIRm2m1.numpy()\n",
    "STIRm1m2 = STIRm1m2.to(\"cpu\")\n",
    "STIRm1m2 = STIRm1m2.numpy()\n",
    "CKA = CKA .to(\"cpu\")\n",
    "CKA = CKA .numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f04bd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('STIRm2m1:', STIRm2m1)\n",
    "print('STIRm1m2:', STIRm1m2)\n",
    "print('CKA:', CKA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42398d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print('STIRm2m1: ', np.round(STIRm2m1, 2))\n",
    "print('STIRm1m2: ', np.round(STIRm1m2, 2))\n",
    "print('CKA: ', np.round(CKA, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1f7198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xs = np.arange(13)\n",
    "plt.rcParams[\"figure.figsize\"] = [10,8]\n",
    "\n",
    "series = np.array(STIRm2m1)\n",
    "smask = np.isfinite(series)\n",
    "plt.plot(xs[smask], series[smask], linestyle='-', marker='s', markersize=10, color='green')\n",
    "\n",
    "series = np.array(STIRm1m2)\n",
    "smask = np.isfinite(series)\n",
    "plt.plot(xs[smask], series[smask], linestyle='-', marker='o', markersize=10, color='blue')\n",
    "\n",
    "series = np.array(CKA)\n",
    "smask = np.isfinite(series)\n",
    "plt.plot(xs[smask], series[smask], linestyle='-', marker='p', markersize=10, color='red')\n",
    "\n",
    "plt.legend(['STIR(Fine-tuned|Pre-trained)', 'STIR(Pre-trained|Fine-tuned)', 'CKA(Pre-trained, Fine-tuned)'], fontsize=20)\n",
    "plt.xlabel('Layer Depth', fontsize = 24, fontweight = 'bold')\n",
    "plt.ylabel('CKA/STIR', fontsize = 24, fontweight ='bold')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n",
    "plt.savefig('CoLA STIR.pdf', bbox_inches='tight')\n",
    "plt.savefig('CoLA STIR.jpg', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
