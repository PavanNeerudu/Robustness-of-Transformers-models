{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23d6854a",
   "metadata": {},
   "source": [
    "### Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399bbf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report, accuracy_score, matthews_corrcoef\n",
    "from transformers import (set_seed,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          GPT2Config,\n",
    "                          GPT2Tokenizer,\n",
    "                          AdamW, \n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          GPT2ForSequenceClassification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f58139",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48deb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('glue', 'cola')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d394d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(123)\n",
    "epochs = 3\n",
    "\n",
    "batch_size = 32\n",
    "max_length = 120\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name_or_path = 'gpt2'\n",
    "\n",
    "labels_ids = {'neg': 0, 'pos': 1}\n",
    "\n",
    "n_labels = len(labels_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d119f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gpt2ClassificationCollator(object):\n",
    "    def __init__(self, use_tokenizer,  max_sequence_len=None):\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
    "\n",
    "    def __call__(self, sequences):\n",
    "        sentences = [sequence['sentence'] for sequence in sequences]\n",
    "        labels = [sequence['label'] for sequence in sequences]\n",
    "        inputs = self.use_tokenizer(text=sentences, return_tensors=\"pt\", padding=True, truncation=True,  max_length=self.max_sequence_len)\n",
    "        inputs.update({'labels':torch.tensor(labels)})\n",
    "        return inputs\n",
    "\n",
    "\n",
    "def train(dataloader, optimizer_, scheduler_, device_):\n",
    "    global model\n",
    "    predictions_labels = []\n",
    "    true_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "\n",
    "        batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss, logits = outputs[:2]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer_.step()\n",
    "        scheduler_.step()\n",
    "\n",
    "        predictions_labels.extend(torch.argmax(logits, dim = 1).tolist())\n",
    "\n",
    "    avg_epoch_loss = total_loss / len(dataloader)\n",
    "    return true_labels, predictions_labels, avg_epoch_loss\n",
    "\n",
    "\n",
    "def validation(dataloader, device_):\n",
    "    global model\n",
    "    predictions_labels = []\n",
    "    true_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "        batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**batch)\n",
    "\n",
    "            loss, logits = outputs[:2] \n",
    "            #logits = logits.detach().cpu().numpy()\n",
    "            total_loss += loss.item()\n",
    "            predict_content = torch.argmax(logits, dim = 1).tolist()\n",
    "            predictions_labels.extend(predict_content)\n",
    "\n",
    "    avg_epoch_loss = total_loss / len(dataloader)\n",
    "    return true_labels, predictions_labels, avg_epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d304437",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cdc8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model configuration.\n",
    "print('Loading configuraiton...')\n",
    "model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path=model_name_or_path, num_labels=n_labels)\n",
    "\n",
    "# Get model's tokenizer.\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path=model_name_or_path)\n",
    "# default to left padding\n",
    "tokenizer.padding_side = \"right\"\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print('Loading model...')\n",
    "model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path, config=model_config)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.to(device)\n",
    "print('Model loaded to `%s`'%device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f2e153",
   "metadata": {},
   "source": [
    "### Creating train, validation and test dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994c05a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator to encode text and labels into numbers.\n",
    "gpt2_classificaiton_collator = Gpt2ClassificationCollator(use_tokenizer=tokenizer,  \n",
    "                                                          max_sequence_len=max_length)\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "train_dataloader = DataLoader(dataset['train'], batch_size=batch_size, shuffle=True, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `train_dataloader` with %d batches!'%len(train_dataloader))\n",
    "\n",
    "print()\n",
    "\n",
    "valid_dataloader = DataLoader(dataset['validation'], batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `val_dataloader` with %d batches!'%len(valid_dataloader))\n",
    "\n",
    "test_dataloader = DataLoader(dataset['test'], batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `test_dataloader` with %d batches!'%len(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b768c8",
   "metadata": {},
   "source": [
    "### Fine-tuning GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ce955",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "all_loss = {'train_loss':[], 'val_loss':[]}\n",
    "all_acc = {'train_acc':[], 'val_acc':[]}\n",
    "\n",
    "# Loop through each epoch.\n",
    "print('Epoch')\n",
    "best_valid_acc = 0\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print()\n",
    "    print('Training on batches...')\n",
    "    # Perform one full pass over the training set.\n",
    "    train_labels, train_predict, train_loss = train(train_dataloader, optimizer, scheduler, device)\n",
    "    train_acc = accuracy_score(train_labels, train_predict)\n",
    "\n",
    "    # Get prediction form model on validation data. \n",
    "    print('Validation on batches...')\n",
    "    valid_labels, valid_predict, val_loss = validation(valid_dataloader, device)\n",
    "    val_acc = accuracy_score(valid_labels, valid_predict)\n",
    "\n",
    "    # Print loss and accuracy values to see how training evolves.\n",
    "    print(\"  train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f\"%(train_loss, val_loss, train_acc, val_acc))\n",
    "    print()\n",
    "\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    all_loss['train_loss'].append(train_loss)\n",
    "    all_loss['val_loss'].append(val_loss)\n",
    "    all_acc['train_acc'].append(train_acc)\n",
    "    all_acc['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Check if the current model is the best so far\n",
    "    if val_acc > best_valid_acc:\n",
    "        best_valid_acc = val_acc\n",
    "        model.save_pretrained(\"gpt2-cola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a3718",
   "metadata": {},
   "outputs": [],
   "source": [
    "preTrainedModel = GPT2ForSequenceClassification.from_pretrained('gpt2')\n",
    "preTrainedModel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf40c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fineTunedModel = GPT2ForSequenceClassification.from_pretrained('gpt2-cola')\n",
    "fineTunedModel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937597ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44124eba",
   "metadata": {},
   "source": [
    "### Obtaining hidden states averaged over all tokens to get a single dimension of 768 for each example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66448513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHiddenStates(currModel, dataloader):\n",
    "    currModel.eval()\n",
    "    totalHiddenStates = [0]*13\n",
    "    flag = True\n",
    "    \n",
    "    # Evaluate data for one epoch\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        #token_type_ids = batch['token_type_ids'].to(device)\n",
    "        \n",
    "        with torch.no_grad():       \n",
    "            outputs = currModel(input_ids=input_ids, \n",
    "                                     attention_mask=attention_mask, \n",
    "                                     output_hidden_states=True, \n",
    "                                     return_dict=True)\n",
    "            hidden_states = outputs.hidden_states\n",
    "            if flag == True:\n",
    "                for i in range(len(hidden_states)):\n",
    "                    totalHiddenStates[i] = hidden_states[i].to(\"cpu\").mean(dim = 1)\n",
    "                flag = False\n",
    "            else:\n",
    "                for i in range(len(hidden_states)):\n",
    "                    temp = hidden_states[i].to(\"cpu\").mean(dim = 1)\n",
    "                    totalHiddenStates[i] = torch.cat((totalHiddenStates[i], temp), 0)   \n",
    "    return totalHiddenStates     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed558f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddenStates = getHiddenStates(model, test_dataloader)\n",
    "preTrainedHiddenStates = torch.stack(hiddenStates, dim = 0)\n",
    "torch.save(preTrainedHiddenStates, 'preTrainedHiddenStates_CoLA.pt')\n",
    "preTrainedHiddenStates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291373c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddenStates = getHiddenStates( fineTunedModel, test_dataloader)\n",
    "fineTunedHiddenStates = torch.stack(hiddenStates, dim = 0)\n",
    "torch.save(fineTunedHiddenStates, 'fineTunedHiddenStates_CoLA.pt')\n",
    "fineTunedHiddenStates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c353eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CKA import CKA, CudaCKA\n",
    "cuda_cka = CudaCKA(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b527650",
   "metadata": {},
   "outputs": [],
   "source": [
    "cka_matrix = torch.empty((13, 13))\n",
    "for i in range(13):\n",
    "    for j in range(13):\n",
    "        X = preTrainedHiddenStates[i].to(device)\n",
    "        Y = fineTunedHiddenStates[j].to(device)\n",
    "        cka_matrix[i][j] = cuda_cka.linear_CKA(X, Y)\n",
    "#torch.save(cka_matrix, 'ckaMatrix.pt')\n",
    "cka_matrix = cka_matrix.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbfffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(cka_matrix, cmap=\"inferno\", vmin=0.0, vmax=1.0)\n",
    "\n",
    "plt.xticks(np.arange(0,13), fontsize = 18)\n",
    "plt.yticks(np.arange(0,13), fontsize = 18)\n",
    "\n",
    "cka_matrix = np.round(cka_matrix, 2)\n",
    "for x in range(13):\n",
    "    for y in range(13):\n",
    "        plt.annotate(cka_matrix[x][y], xy=(y, x), horizontalalignment='center', verticalalignment='center', fontsize=13)\n",
    "\n",
    "plt.ylabel('Pre-trained GPT-2', fontsize = 22)\n",
    "plt.xlabel('Fine-tuned GPT-2', fontsize = 22)\n",
    "\n",
    "#fig.colorbar(im)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "plt.savefig('CoLA_preTrained_fineTuned.pdf',  bbox_inches='tight')\n",
    "plt.savefig('CoLA_preTrained_fineTuned.jpg',  bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff9db29",
   "metadata": {},
   "source": [
    "### Calculating layerwise STIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a8eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preTrainedHiddenStates = torch.load('./preTrainedHiddenStates_CoLA.pt').to(device)\n",
    "fineTunedHiddenStates  = torch.load('./fineTunedHiddenStates_CoLA.pt').to(device)\n",
    "print(preTrainedHiddenStates.shape,fineTunedHiddenStates.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e6b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinm1 = []\n",
    "distinm2 = []\n",
    "for i in range(13):\n",
    "    distinm1.append(torch.cdist(preTrainedHiddenStates[i], preTrainedHiddenStates[i]))\n",
    "    distinm2.append(torch.cdist(fineTunedHiddenStates[i], fineTunedHiddenStates[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7868cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(13):\n",
    "    for j in range(distinm1[0].shape[0]):\n",
    "        distinm1[i][j][j] = 100000.0\n",
    "        distinm2[i][j][j] = 100000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2728ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = preTrainedHiddenStates.shape[1]\n",
    "minDistIndForm1 = []\n",
    "for i in range(13):\n",
    "    temp = []\n",
    "    for j in range(num_rows):\n",
    "        temp.append(torch.argmin(distinm1[i][j]))\n",
    "    minDistIndForm1.append(temp)\n",
    "    \n",
    "minDistIndForm2 = []\n",
    "for i in range(13):\n",
    "    temp = []\n",
    "    for j in range(num_rows):\n",
    "        temp.append(torch.argmin(distinm2[i][j]))\n",
    "    minDistIndForm2.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f35ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CKA import CKA, CudaCKA\n",
    "cuda_cka = CudaCKA(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7d5234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def calculate(noOfSamples, noOfTimes):\n",
    "    STIRm2m1 = []\n",
    "    STIRm1m2 = []\n",
    "    CKA = []\n",
    "    for i in range(13):\n",
    "        stirm2m1 = 0\n",
    "        stirm1m2 = 0\n",
    "        cka = 0\n",
    "        for j in range(noOfTimes):\n",
    "            num_list = random.sample(range(0, num_rows), noOfSamples)\n",
    "            X = []\n",
    "            X_ = []\n",
    "            Y = []\n",
    "            Y_ = []\n",
    "            for k in range(len(num_list)):\n",
    "                X.append(fineTunedHiddenStates[i][num_list[k]])\n",
    "                X_.append(fineTunedHiddenStates[i][minDistIndForm1[i][num_list[k]]])\n",
    "                Y.append(preTrainedHiddenStates[i][num_list[k]])\n",
    "                Y_.append(preTrainedHiddenStates[i][minDistIndForm2[i][num_list[k]]])\n",
    "                \n",
    "            X = torch.stack(X, dim = 0)\n",
    "            X_ = torch.stack(X_, dim = 0)\n",
    "            Y = torch.stack(Y, dim = 0)\n",
    "            Y_ = torch.stack(Y_, dim = 0)\n",
    "            \n",
    "            stirm2m1 = stirm2m1 + cuda_cka.linear_CKA(X, X_)\n",
    "            stirm1m2 = stirm1m2 + cuda_cka.linear_CKA(Y, Y_)\n",
    "            cka = cka + cuda_cka.linear_CKA(Y, X)\n",
    "        \n",
    "        STIRm2m1.append(stirm2m1/noOfTimes)\n",
    "        STIRm1m2.append(stirm1m2/noOfTimes)\n",
    "        CKA.append(cka/noOfTimes)\n",
    "    return STIRm2m1, STIRm1m2, CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260681cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "STIRm2m1, STIRm1m2, CKA = calculate(num_rows//2, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98633f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "STIRm2m1 = torch.stack(STIRm2m1, dim = 0)\n",
    "STIRm1m2 = torch.stack(STIRm1m2,dim = 0)\n",
    "CKA = torch.stack(CKA, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7924cdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "STIRm2m1 = STIRm2m1.to(\"cpu\")\n",
    "STIRm2m1 = STIRm2m1.numpy()\n",
    "STIRm1m2 = STIRm1m2.to(\"cpu\")\n",
    "STIRm1m2 = STIRm1m2.numpy()\n",
    "CKA = CKA .to(\"cpu\")\n",
    "CKA = CKA .numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f04bd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('STIRm2m1:', STIRm2m1)\n",
    "print('STIRm1m2:', STIRm1m2)\n",
    "print('CKA:', CKA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42398d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print('STIRm2m1: ', np.round(STIRm2m1, 2))\n",
    "print('STIRm1m2: ', np.round(STIRm1m2, 2))\n",
    "print('CKA: ', np.round(CKA, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1f7198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xs = np.arange(13)\n",
    "plt.rcParams[\"figure.figsize\"] = [10,8]\n",
    "\n",
    "series = np.array(STIRm2m1)\n",
    "smask = np.isfinite(series)\n",
    "plt.plot(xs[smask], series[smask], linestyle='-', marker='s', markersize=10, color='green')\n",
    "\n",
    "series = np.array(STIRm1m2)\n",
    "smask = np.isfinite(series)\n",
    "plt.plot(xs[smask], series[smask], linestyle='-', marker='o', markersize=10, color='blue')\n",
    "\n",
    "series = np.array(CKA)\n",
    "smask = np.isfinite(series)\n",
    "plt.plot(xs[smask], series[smask], linestyle='-', marker='p', markersize=10, color='red')\n",
    "\n",
    "plt.legend(['STIR(Fine-tuned|Pre-trained)', 'STIR(Pre-trained|Fine-tuned)', 'CKA(Pre-trained, Fine-tuned)'], fontsize=20)\n",
    "plt.xlabel('Layer Depth', fontsize = 24, fontweight = 'bold')\n",
    "plt.ylabel('CKA/STIR', fontsize = 24, fontweight ='bold')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n",
    "plt.savefig('CoLA STIR.pdf', bbox_inches='tight')\n",
    "plt.savefig('CoLA STIR.jpg', bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd950a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c059d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, device_):\n",
    "    predictions_labels = []\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        #token_type_ids = batch['token_type_ids'].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "            logits = outputs.logits\n",
    "            #logits = logits.detach().cpu().numpy()\n",
    "            predict_content = torch.argmax(logits, dim = 1).tolist()\n",
    "            predictions_labels.extend(predict_content)\n",
    "\n",
    "    return predictions_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a432575",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            'sentence': self.sentences[index],\n",
    "            'label':0,\n",
    "        }\n",
    "\n",
    "test_dataset = Dataset(test_sentences)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `test_dataloader` with %d batches!'%len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84eee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, device_):\n",
    "    predictions_labels = []\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        #token_type_ids = batch['token_type_ids'].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "            logits = outputs.logits\n",
    "            #logits = logits.detach().cpu().numpy()\n",
    "            predict_content = torch.argmax(logits, dim = 1).tolist()\n",
    "            predictions_labels.extend(predict_content)\n",
    "\n",
    "    return predictions_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7175ff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "predictions = test(test_dataloader, device)\n",
    "filename = '../swapWords/CoLA.tsv'\n",
    "result = pd.DataFrame(predictions, columns=['prediction'])\n",
    "result.insert(0, 'index', range(0, len(result)))\n",
    "result.to_csv(filename, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deec970",
   "metadata": {},
   "source": [
    "### Removing first word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb19656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [sentence.split(\" \", 1)[1] if \" \" in sentence else sentence for sentence in dataset['test']['sentence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bfbc17",
   "metadata": {},
   "source": [
    "### Removing last word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf2e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [sentence.rsplit(\" \", 1)[0] + sentence[-1] for sentence in dataset['test']['sentence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72871570",
   "metadata": {},
   "source": [
    "### Removing nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03509e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "\n",
    "def remove_nouns(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    tagged_words = pos_tag(words)\n",
    "    filtered_sentence = [word for word, pos in tagged_words if pos != 'NN' and pos != 'NNP' and pos != 'NNS']\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "test_sentences = [remove_nouns(sentence) for sentence in dataset['test']['sentence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ec9370",
   "metadata": {},
   "source": [
    "### Removing verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008ac853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_verbs(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    tagged_words = pos_tag(words)\n",
    "    filtered_sentence = [word for word, pos in tagged_words if pos != 'VB' and pos != 'VBD' and pos != 'VBG' and pos != 'VBN' and pos != 'VBP' and pos != 'VBZ']\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "test_sentences = [remove_verbs(sentence) for sentence in dataset['test']['sentence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf537e37",
   "metadata": {},
   "source": [
    "### Swap text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5161ba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def random_swap_words(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    if len(words) >= 2:\n",
    "        i, j = random.sample(range(len(words)), 2)\n",
    "        words[i], words[j] = words[j], words[i]\n",
    "        return \" \".join(words)\n",
    "    else:\n",
    "        return sentence\n",
    "    \n",
    "test_sentences = [random_swap_words(sentence) for sentence in dataset['test']['sentence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46b2502",
   "metadata": {},
   "source": [
    "### Add text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb2f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from faker import Faker\n",
    "from typing import List\n",
    "\n",
    "def add_random_words(sentences: List[str]):\n",
    "    fake = Faker()\n",
    "    new_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        num_words_to_add = int(len(words) / 5)\n",
    "        for i in range(num_words_to_add):\n",
    "            insert_index = random.randint(0, len(words) - 1)\n",
    "            words.insert(insert_index, fake.word())\n",
    "        new_sentences.append(\" \".join(words))\n",
    "    return new_sentences\n",
    "\n",
    "test_sentences = add_random_words(dataset['test']['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7316a73a",
   "metadata": {},
   "source": [
    "### Change char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d540da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "def replace_characters(sentence, probability=0.10):\n",
    "    new_sentence = \"\"\n",
    "    for char in sentence:\n",
    "        if random.random() < probability:\n",
    "            char = random.choice(string.ascii_letters)\n",
    "        new_sentence += char\n",
    "    return new_sentence\n",
    "\n",
    "test_sentences = [replace_characters(sentence) for sentence in dataset['test']['sentence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a3dc57",
   "metadata": {},
   "source": [
    "### Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d613287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeGender(str):\n",
    "    dictionary = {\n",
    "    \"batman\": \"batwoman\", \"batwoman\": \"batman\",\n",
    "    \"boy\": \"girl\", \"girl\": \"boy\",\n",
    "    \"boyfriend\": \"girlfriend\", \"girlfriend\": \"boyfriend\",\n",
    "    \"father\": \"mother\", \"mother\": \"father\",\n",
    "    \"husband\": \"wife\", \"wife\": \"husband\",\n",
    "    \"he\": \"she\", \"she\": \"he\", \"He\":\"She\", \"She\":\"He\",\n",
    "    \"his\": \"her\", \"her\": \"his\", \"His\":\"Her\", \"Her\":\"His\",\n",
    "    \"male\": \"female\", \"female\": \"male\",\"him\":\"her\",\"her\":\"him\",\n",
    "    \"man\": \"woman\", \"woman\": \"man\",\n",
    "    \"Mr\": \"Ms\", \"Mr\": \"Ms\",\n",
    "    \"sir\": \"madam\", \"madam\": \"sir\",\n",
    "    \"son\": \"daughter\", \"daughter\": \"son\",\n",
    "    \"uncle\": \"aunt\", \"aunt\": \"uncle\",\n",
    "    }\n",
    "\n",
    "    str = str + ' ' # Append a space at the en\n",
    "    temp = \"\"\n",
    "    ans = \"\"\n",
    "\n",
    "    for i in range(len(str)):\n",
    "        if str[i] != ' ':\n",
    "            temp += str[i]\n",
    "        else:\n",
    "            if temp in dictionary:\n",
    "                temp = dictionary[temp]\n",
    "\n",
    "            ans = ans + temp + ' '\n",
    "            temp = \"\"\n",
    "\n",
    "    return ans\n",
    "\n",
    "test_sentences = [changeGender(sentence) for sentence in dataset['test']['sentence']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
