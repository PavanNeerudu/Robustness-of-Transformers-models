{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17556d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA RTX A4000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5d49b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moved model to cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"p208p2002/gpt2-squad-qg-hl\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"p208p2002/gpt2-squad-qg-hl\")\n",
    "model.to(device)\n",
    "print(\"moved model to cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "522c29a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (C:/Users/NIT/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5741a700b93d4c9aa31813fb8187429c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"squad\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1535f7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Sentences: 100%|██████████████████████████████████████████████████████████| 331/331 [13:20<00:00,  2.42s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging \n",
    "# Set the logging level to suppress warning messages\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "temperature = 1.0\n",
    "k = 0\n",
    "p = 0.9\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side='left'\n",
    "\n",
    "\n",
    "def gpt2_gen_sentences(words_list, max_length=1024, batch_size=32):\n",
    "    input_texts = words_list\n",
    "    num_batches = len(input_texts) // batch_size + 1\n",
    "    predictions = []\n",
    "    \n",
    "    with tqdm(total=num_batches, desc='Generating Sentences') as pbar:\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = (i + 1) * batch_size\n",
    "            batch_texts = input_texts[start_idx:end_idx]\n",
    "            \n",
    "            if len(batch_texts) > 0:\n",
    "                features = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "                input_ids = features['input_ids'].to(device)\n",
    "                attention_mask = features['attention_mask'].to(device)\n",
    "                outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_length,\n",
    "                                         temperature = temperature, repetition_penalty = 1.0,\n",
    "                                         top_k = k, top_p = p, num_return_sequences=1)\n",
    "                \n",
    "                \n",
    "                batch_predictions = []\n",
    "                for output in outputs:\n",
    "                    decoded_output = tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "                    generated_tokens = decoded_output.split(' ')\n",
    "                    question_tokens = []\n",
    "                    hl_count = 0\n",
    "                    for token in generated_tokens:\n",
    "                        if token == '[HL]':\n",
    "                            hl_count += 1\n",
    "                            if hl_count == 2:\n",
    "                                break\n",
    "                        elif hl_count == 2:\n",
    "                            question_tokens.append(token)\n",
    "                    question = ' '.join(question_tokens)\n",
    "                    batch_predictions.append(question)\n",
    "\n",
    "                predictions.extend(batch_predictions)\n",
    "            pbar.update(1)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "inputs = []\n",
    "for i in range(dataset['validation'].num_rows): \n",
    "    inputdata = dataset['validation'][i]['context'] + '[HL]' + dataset['validation'][i]['answers']['text'][0] + '[HL]'\n",
    "    inputs.append(inputdata)\n",
    "\n",
    "predictions = gpt2_gen_sentences(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6db70128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging \n",
    "# Set the logging level to suppress warning messages\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "temperature = 1.0\n",
    "k = 0\n",
    "p = 0.9\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side='left'\n",
    "\n",
    "\n",
    "def gpt2_gen_sentences(words_list, max_length=512, batch_size=32):\n",
    "    input_texts = words_list\n",
    "    num_batches = len(input_texts) // batch_size + 1\n",
    "    predictions = []\n",
    "    \n",
    "    with tqdm(total=num_batches, desc='Generating Sentences') as pbar:\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = (i + 1) * batch_size\n",
    "            batch_texts = input_texts[start_idx:end_idx]\n",
    "            \n",
    "            if len(batch_texts) > 0:\n",
    "                features = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "                input_ids = features['input_ids'].to(device)\n",
    "                attention_mask = features['attention_mask'].to(device)\n",
    "                outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_length,\n",
    "                                         temperature = temperature, repetition_penalty = 1.0,\n",
    "                                         top_k = k, top_p = p, num_return_sequences=1)\n",
    "                \n",
    "                \n",
    "                batch_predictions = []\n",
    "                for output in outputs:\n",
    "                    decoded_output = tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "                    last_hl_index = decoded_output.rfind('[HL]')\n",
    "                    question = decoded_output[last_hl_index + len('[HL]'):].strip()\n",
    "                    batch_predictions.append(question)\n",
    "                predictions.extend(batch_predictions)\n",
    "            pbar.update(1)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bcb5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for i in range(dataset['validation'].num_rows): \n",
    "    inputdata = dataset['validation'][i]['context'] + '[HL]' + dataset['validation'][i]['answers']['text'][0] + '[HL]'\n",
    "    inputs.append(inputdata)\n",
    "\n",
    "predictions = gpt2_gen_sentences(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97fb2cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 Score: 0.2615455413346295\n",
      "Average ROUGE-2 Score: 0.08492763221916563\n",
      "Average ROUGE-L Score: 0.24912288623178772\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "rouge_scorer = Rouge()\n",
    "\n",
    "# Replace empty predictions with \"?\"\n",
    "predictions = [prediction if prediction else \"?\" for prediction in predictions]\n",
    "\n",
    "# Calculate ROUGE scores for each sentence\n",
    "rouge_scores = rouge_scorer.get_scores(predictions, dataset['validation']['question'])\n",
    "\n",
    "# Calculate average ROUGE scores\n",
    "rouge_1_scores = [scores['rouge-1']['f'] for scores in rouge_scores]\n",
    "rouge_2_scores = [scores['rouge-2']['f'] for scores in rouge_scores]\n",
    "rouge_l_scores = [scores['rouge-l']['f'] for scores in rouge_scores]\n",
    "\n",
    "avg_rouge_1 = sum(rouge_1_scores) / len(rouge_1_scores)\n",
    "avg_rouge_2 = sum(rouge_2_scores) / len(rouge_2_scores)\n",
    "avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)\n",
    "\n",
    "print(\"Average ROUGE-1 Score:\", avg_rouge_1)\n",
    "print(\"Average ROUGE-2 Score:\", avg_rouge_2)\n",
    "print(\"Average ROUGE-L Score:\", avg_rouge_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "959e0325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Sentences: 100%|██████████████████████████████████████████████████████████| 331/331 [20:57<00:00,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbation: noNouns\n",
      "Average ROUGE-1 Score: 0.20557141750747882\n",
      "Average ROUGE-2 Score: 0.05415378444200015\n",
      "Average ROUGE-L Score: 0.1968495103745301\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Sentences: 100%|██████████████████████████████████████████████████████████| 331/331 [07:45<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbation: noVerbs\n",
      "Average ROUGE-1 Score: 0.23397444026505781\n",
      "Average ROUGE-2 Score: 0.07318150639206751\n",
      "Average ROUGE-L Score: 0.2231810095808056\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Sentences: 100%|██████████████████████████████████████████████████████████| 331/331 [07:10<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbation: noFirst\n",
      "Average ROUGE-1 Score: 0.2609917437456836\n",
      "Average ROUGE-2 Score: 0.08400809879570323\n",
      "Average ROUGE-L Score: 0.24857285896995027\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Sentences: 100%|██████████████████████████████████████████████████████████| 331/331 [07:22<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbation: noLast\n",
      "Average ROUGE-1 Score: 0.2602802079521063\n",
      "Average ROUGE-2 Score: 0.08423108952140376\n",
      "Average ROUGE-L Score: 0.24798403683678524\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Sentences: 100%|██████████████████████████████████████████████████████████| 331/331 [07:06<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbation: swapText\n",
      "Average ROUGE-1 Score: 0.2385366578621786\n",
      "Average ROUGE-2 Score: 0.07921936254384164\n",
      "Average ROUGE-L Score: 0.22703071850693443\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Sentences: 100%|██████████████████████████████████████████████████████████| 331/331 [09:13<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbation: addText\n",
      "Average ROUGE-1 Score: 0.25353139937892866\n",
      "Average ROUGE-2 Score: 0.07909825314128967\n",
      "Average ROUGE-L Score: 0.2421560476183898\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Sentences: 100%|██████████████████████████████████████████████████████████| 331/331 [12:13<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbation: changeChar\n",
      "Average ROUGE-1 Score: 0.22282465527808878\n",
      "Average ROUGE-2 Score: 0.058804071114475207\n",
      "Average ROUGE-L Score: 0.21446997729919226\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Sentences: 100%|██████████████████████████████████████████████████████████| 331/331 [07:14<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbation: bias\n",
      "Average ROUGE-1 Score: 0.2520320926154008\n",
      "Average ROUGE-2 Score: 0.08224004497217495\n",
      "Average ROUGE-L Score: 0.23946002818860276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from rouge import Rouge\n",
    "\n",
    "\n",
    "\n",
    "pertNames = [\"noNouns\", \"noVerbs\", \"noFirst\", \"noLast\", \"swapText\", \"addText\", \"changeChar\", \"bias\"]\n",
    "\n",
    "for pert in pertNames:\n",
    "    valDs = load_from_disk('../Datasets/'+'squad'+'validation'+pert)\n",
    "    inputs = []\n",
    "    for i in range(len(valDs['context'])): \n",
    "        inputdata = valDs[i]['context'] + '[HL]' + valDs[i]['answers']['text'][0] + '[HL]'\n",
    "        inputs.append(inputdata)\n",
    "\n",
    "    predictions = gpt2_gen_sentences(inputs)\n",
    "    # Replace empty predictions with \"?\"\n",
    "    predictions = [prediction if prediction else \"?\" for prediction in predictions]\n",
    "\n",
    "    \n",
    "    rouge_scorer = Rouge()\n",
    "\n",
    "    # Calculate ROUGE scores for each sentence\n",
    "    rouge_scores = rouge_scorer.get_scores(predictions, valDs['question'])\n",
    "\n",
    "    # Calculate average ROUGE scores\n",
    "    rouge_1_scores = [scores['rouge-1']['f'] for scores in rouge_scores]\n",
    "    rouge_2_scores = [scores['rouge-2']['f'] for scores in rouge_scores]\n",
    "    rouge_l_scores = [scores['rouge-l']['f'] for scores in rouge_scores]\n",
    "\n",
    "    avg_rouge_1 = sum(rouge_1_scores) / len(rouge_1_scores)\n",
    "    avg_rouge_2 = sum(rouge_2_scores) / len(rouge_2_scores)\n",
    "    avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)\n",
    "\n",
    "    print(\"Perturbation:\", pert)\n",
    "    print(\"Average ROUGE-1 Score:\", avg_rouge_1)\n",
    "    print(\"Average ROUGE-2 Score:\", avg_rouge_2)\n",
    "    print(\"Average ROUGE-L Score:\", avg_rouge_l)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e920d12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 124442112\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total Parameters:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792c5fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048fd96e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "218e3bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1a09e963a34ccf9fd9538872adcdea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25dbb94a03ec4edb974e753dd5030010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650b5dcbcc4c4dd1864d6eeb96e61ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842c58bf1b4244abbd7c5714872dae74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e75b17ce6b44afb77989c9fbc7ea08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moved model to  cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "model.to(device)\n",
    "print(\"moved model to \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4dbf2073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Sentences: 100%|██████████████████████████████████████████████████████████| 331/331 [05:34<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "def gen_sentences(words_list, max_length=128, batch_size=32):\n",
    "    input_texts = words_list\n",
    "    num_batches = len(input_texts) // batch_size + 1\n",
    "    predictions = []\n",
    "    \n",
    "    with tqdm(total=num_batches, desc='Generating Sentences') as pbar:\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = (i + 1) * batch_size\n",
    "            batch_texts = input_texts[start_idx:end_idx]\n",
    "            \n",
    "            if len(batch_texts) > 0:\n",
    "                features = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "                input_ids = features['input_ids'].to(device)\n",
    "                attention_mask = features['attention_mask'].to(device)\n",
    "                outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_length)\n",
    "                batch_predictions = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "                predictions.extend(batch_predictions)\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "inputs = []\n",
    "for i in range(dataset['validation'].num_rows): \n",
    "    inputdata = 'answer: ' + dataset['validation'][i]['answers']['text'][0] + 'context' + dataset['validation'][i]['context']\n",
    "    inputs.append(inputdata)\n",
    "    \n",
    "predictions = gen_sentences(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0aedfa8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 Score: 0.41241320036432605\n",
      "Average ROUGE-2 Score: 0.22388853611313164\n",
      "Average ROUGE-L Score: 0.3963288135971544\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "# Initialize ROUGE scorer\n",
    "rouge_scorer = Rouge()\n",
    "\n",
    "# Calculate ROUGE scores for each sentence\n",
    "rouge_scores = rouge_scorer.get_scores(predictions, dataset['validation']['question'])\n",
    "\n",
    "# Calculate average ROUGE scores\n",
    "rouge_1_scores = [scores['rouge-1']['f'] for scores in rouge_scores]\n",
    "rouge_2_scores = [scores['rouge-2']['f'] for scores in rouge_scores]\n",
    "rouge_l_scores = [scores['rouge-l']['f'] for scores in rouge_scores]\n",
    "\n",
    "avg_rouge_1 = sum(rouge_1_scores) / len(rouge_1_scores)\n",
    "avg_rouge_2 = sum(rouge_2_scores) / len(rouge_2_scores)\n",
    "avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)\n",
    "\n",
    "print(\"Average ROUGE-1 Score:\", avg_rouge_1)\n",
    "print(\"Average ROUGE-2 Score:\", avg_rouge_2)\n",
    "print(\"Average ROUGE-L Score:\", avg_rouge_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44aa1e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Sentences: 100%|██████████████████████████████████████████████████████████| 331/331 [05:32<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbation: noNouns\n",
      "Average ROUGE-1 Score: 0.262085939512435\n",
      "Average ROUGE-2 Score: 0.08663498674685655\n",
      "Average ROUGE-L Score: 0.2515729685356585\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Sentences: 100%|██████████████████████████████████████████████████████████| 331/331 [05:30<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbation: noVerbs\n",
      "Average ROUGE-1 Score: 0.36504205758650093\n",
      "Average ROUGE-2 Score: 0.17391520343838945\n",
      "Average ROUGE-L Score: 0.34955748519491414\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Sentences: 100%|██████████████████████████████████████████████████████████| 331/331 [05:27<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbation: noFirst\n",
      "Average ROUGE-1 Score: 0.38857225890797503\n",
      "Average ROUGE-2 Score: 0.20274750801523092\n",
      "Average ROUGE-L Score: 0.37337256117193396\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Sentences: 100%|██████████████████████████████████████████████████████████| 331/331 [05:28<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbation: noLast\n",
      "Average ROUGE-1 Score: 0.4108130805428246\n",
      "Average ROUGE-2 Score: 0.22309482329888874\n",
      "Average ROUGE-L Score: 0.3948261226061751\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Sentences: 100%|██████████████████████████████████████████████████████████| 331/331 [05:43<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbation: swapText\n",
      "Average ROUGE-1 Score: 0.40446087630407324\n",
      "Average ROUGE-2 Score: 0.2156343039723088\n",
      "Average ROUGE-L Score: 0.3884155529947696\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Sentences: 100%|██████████████████████████████████████████████████████████| 331/331 [06:22<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbation: addText\n",
      "Average ROUGE-1 Score: 0.3984876562845445\n",
      "Average ROUGE-2 Score: 0.20681129802158715\n",
      "Average ROUGE-L Score: 0.383188876285867\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Sentences: 100%|██████████████████████████████████████████████████████████| 331/331 [09:04<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbation: changeChar\n",
      "Average ROUGE-1 Score: 0.28476758165095445\n",
      "Average ROUGE-2 Score: 0.10962850787422557\n",
      "Average ROUGE-L Score: 0.27533632405897207\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Sentences: 100%|██████████████████████████████████████████████████████████| 331/331 [12:36<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbation: bias\n",
      "Average ROUGE-1 Score: 0.41199576036413355\n",
      "Average ROUGE-2 Score: 0.2236740628904512\n",
      "Average ROUGE-L Score: 0.39602025233789434\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "## Perturbations applied on the context and not on the answers\n",
    "pertNames = [\"noNouns\", \"noVerbs\", \"noFirst\", \"noLast\", \"swapText\", \"addText\", \"changeChar\", \"bias\"]\n",
    "\n",
    "for pert in pertNames:\n",
    "    valDs = load_from_disk('../../Datasets/'+'squad'+'validation'+pert)\n",
    "    inputs = []\n",
    "    for i in range(len(valDs['context'])): \n",
    "        inputdata = 'answer: ' + valDs[i]['answers']['text'][0] + 'context' + valDs[i]['context']\n",
    "        inputs.append(inputdata)\n",
    "    \n",
    "    predictions = gen_sentences(inputs)\n",
    "    \n",
    "    rouge_scorer = Rouge()\n",
    "\n",
    "    # Calculate ROUGE scores for each sentence\n",
    "    rouge_scores = rouge_scorer.get_scores(predictions, valDs['question'])\n",
    "\n",
    "    # Calculate average ROUGE scores\n",
    "    rouge_1_scores = [scores['rouge-1']['f'] for scores in rouge_scores]\n",
    "    rouge_2_scores = [scores['rouge-2']['f'] for scores in rouge_scores]\n",
    "    rouge_l_scores = [scores['rouge-l']['f'] for scores in rouge_scores]\n",
    "\n",
    "    avg_rouge_1 = sum(rouge_1_scores) / len(rouge_1_scores)\n",
    "    avg_rouge_2 = sum(rouge_2_scores) / len(rouge_2_scores)\n",
    "    avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)\n",
    "\n",
    "    print(\"Perturbation:\", pert)\n",
    "    print(\"Average ROUGE-1 Score:\", avg_rouge_1)\n",
    "    print(\"Average ROUGE-2 Score:\", avg_rouge_2)\n",
    "    print(\"Average ROUGE-L Score:\", avg_rouge_l)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab6f45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
